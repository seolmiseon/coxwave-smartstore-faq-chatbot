# ê¸€ë¡œë²Œ ì¿¼ë¦¬ ìºì‹± ì„œë¹„ìŠ¤ (FSF ì¶•êµ¬ í”Œë«í¼ ì „ëµ ì ìš©)
# ì½•ìŠ¤ì›¨ì´ë¸Œ ê³¼ì œ ì „í˜•

from openai import OpenAI
import os
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import hashlib
import chromadb

logger = logging.getLogger(__name__)


class QueryCacheService:
    """
    ê¸€ë¡œë²Œ ì¿¼ë¦¬ ìºì‹± ì„œë¹„ìŠ¤ (FSF ì¶•êµ¬ í”Œë«í¼ ì „ëµ ì ìš©)

    í•µì‹¬ ê¸°ëŠ¥:
    1. ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)
    2. ìºì‹œ íˆíŠ¸ ì‹œ LLM í˜¸ì¶œ ìƒëµ â†’ 90% ë¹„ìš© ì ˆê°
    3. ìƒˆ ì‘ë‹µ ìë™ ìºì‹±

    ì‚¬ìš© ì‚¬ë¡€:
        User A: "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì… ë°©ë²• ì•Œë ¤ì¤˜"
        User B: "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì…ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
        User C: "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì‹ ì²­ ì ˆì°¨ëŠ”?"
        â†’ ìœ ì‚¬ë„ 90% ì´ìƒì´ë©´ ê°™ì€ ë‹µë³€ ì¬ì‚¬ìš© (LLM í˜¸ì¶œ 1ë²ˆë§Œ!)
    """

    def __init__(
        self,
        cache_path: str = "./data/chroma_cache",
        similarity_threshold: float = 0.90
    ):
        """
        ì¿¼ë¦¬ ìºì‹œ ì´ˆê¸°í™”

        Args:
            cache_path: ChromaDB ì €ì¥ ê²½ë¡œ
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.90 = 90% ì´ìƒ ìœ ì‚¬)
        """
        self.client = chromadb.PersistentClient(path=cache_path)
        self.cache_collection = self.client.get_or_create_collection(
            name="query_cache",
            metadata={"hnsw:space": "cosine"}  # Cosine ìœ ì‚¬ë„
        )
        self.openai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.threshold = similarity_threshold

        # í†µê³„
        self.cache_hits = 0
        self.cache_misses = 0

        logger.info(f"QueryCacheService ì´ˆê¸°í™” ì™„ë£Œ (threshold: {similarity_threshold})")

    def _get_cache_key(self, query: str) -> str:
        """ì§ˆë¬¸ì„ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜ (ì •í™•íˆ ê°™ì€ ì§ˆë¬¸ íŒë³„)"""
        return hashlib.md5(query.encode()).hexdigest()

    def search_similar_cache(
        self,
        query: str,
        threshold: Optional[float] = None
    ) -> Optional[Dict]:
        """
        ìœ ì‚¬í•œ ìºì‹œëœ ë‹µë³€ ê²€ìƒ‰

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.90)

        Returns:
            ìºì‹œ íˆíŠ¸ ì‹œ:
                {
                    "cached": True,
                    "answer": "ë‹µë³€ ë‚´ìš©",
                    "similarity": 0.95,
                    "original_query": "ì›ë˜ ì§ˆë¬¸",
                    "timestamp": "2025-12-07T...",
                    "follow_up_questions": [...],
                    "sources": [...]
                }
            ìºì‹œ ë¯¸ìŠ¤ ì‹œ: None
        """
        try:
            if threshold is None:
                threshold = self.threshold

            # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
            embedding = (
                self.openai.embeddings.create(
                    model="text-embedding-3-small",
                    input=query
                )
                .data[0]
                .embedding
            )

            # 2. ChromaDBì—ì„œ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰
            results = self.cache_collection.query(
                query_embeddings=[embedding],
                n_results=1
            )

            # 3. ìœ ì‚¬ë„ ì²´í¬
            if results["distances"][0]:
                distance = results["distances"][0][0]
                similarity = 1 - distance  # cosine distance â†’ similarity

                if similarity >= threshold:
                    self.cache_hits += 1
                    metadata = results["metadatas"][0][0]

                    logger.info(
                        f"ğŸ¯ ìºì‹œ HIT! ìœ ì‚¬ë„: {similarity:.2%} "
                        f"(ì›ë˜ ì§ˆë¬¸: {metadata['query']})"
                    )

                    return {
                        "cached": True,
                        "answer": metadata["answer"],
                        "similarity": similarity,
                        "original_query": metadata["query"],
                        "timestamp": metadata.get("timestamp", ""),
                        "follow_up_questions": json.loads(metadata.get("follow_ups", "[]")),
                        "sources": json.loads(metadata.get("sources", "[]"))
                    }
                else:
                    self.cache_misses += 1
                    logger.info(f"ìºì‹œ MISS (ìœ ì‚¬ë„ {similarity:.2%} < {threshold:.2%})")

            return None

        except Exception as e:
            logger.error(f"ìºì‹œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None

    def save_cache(
        self,
        query: str,
        answer: str,
        follow_up_questions: List[str],
        sources: List[Dict[str, Any]]
    ):
        """
        ìƒˆ ë‹µë³€ì„ ìºì‹œì— ì €ì¥

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            answer: LLM ì‘ë‹µ
            follow_up_questions: í›„ì† ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            sources: ì°¸ê³  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
            embedding = (
                self.openai.embeddings.create(
                    model="text-embedding-3-small",
                    input=query
                )
                .data[0]
                .embedding
            )

            # 2. ChromaDBì— ì €ì¥
            cache_key = self._get_cache_key(query)

            self.cache_collection.upsert(
                ids=[cache_key],
                embeddings=[embedding],
                documents=[query],  # ê²€ìƒ‰ìš© (ì‹¤ì œë¡  ì‚¬ìš© ì•ˆí•¨)
                metadatas=[{
                    "query": query,
                    "answer": answer,
                    "follow_ups": json.dumps(follow_up_questions, ensure_ascii=False),
                    "sources": json.dumps(sources, ensure_ascii=False),
                    "timestamp": datetime.now().isoformat()
                }]
            )

            logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {query[:50]}...")

        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def get_cache_stats(self) -> Dict:
        """
        ìºì‹œ í†µê³„ ì¡°íšŒ

        Returns:
            {
                "total_cached": 100,
                "cache_hits": 50,
                "cache_misses": 10,
                "hit_rate": 0.833,
                "cache_path": "./data/chroma_cache"
            }
        """
        try:
            total_requests = self.cache_hits + self.cache_misses
            hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0

            return {
                "total_cached": self.cache_collection.count(),
                "cache_hits": self.cache_hits,
                "cache_misses": self.cache_misses,
                "hit_rate": round(hit_rate, 3)
            }

        except Exception as e:
            logger.error(f"ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                "total_cached": 0,
                "cache_hits": 0,
                "cache_misses": 0,
                "hit_rate": 0.0
            }
