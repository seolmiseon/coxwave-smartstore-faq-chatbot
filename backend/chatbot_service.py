# FAQ ì±—ë´‡ ì„œë¹„ìŠ¤ (OpenAI + Solar Pro í•˜ì´ë¸Œë¦¬ë“œ)
# ì½•ìŠ¤ì›¨ì´ë¸Œ ê³¼ì œ ì „í˜•

from rag_service import RAGService
from solar_service import SolarService
from cache_service import QueryCacheService
from openai import OpenAI
import os
import logging
from typing import List, Dict, Any, Optional, AsyncIterator
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class ChatbotService:
    """
    FAQ ì±—ë´‡ ë©”ì¸ ì„œë¹„ìŠ¤

    ì°¨ë³„í™” í¬ì¸íŠ¸:
    1. OpenAI (ì„ë² ë”©) + Solar Pro (ì±„íŒ…) í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ
    2. Hybrid RAG (Semantic + Keyword + RRF)
    3. ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
    4. í›„ì† ì§ˆë¬¸ ìƒì„±
    5. ë„ë©”ì¸ í•„í„°ë§ (ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì „ìš©)
    """

    def __init__(self):
        """ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.rag_service = RAGService(
            persist_directory=os.getenv("CHROMA_PERSIST_DIRECTORY", "./chroma_db"),
            collection_name="smartstore_faq"
        )

        # ì¿¼ë¦¬ ìºì‹œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ê¸€ë¡œë²Œ ìºì‹± - 90% ë¹„ìš© ì ˆê°)
        cache_threshold = float(os.getenv("CACHE_SIMILARITY_THRESHOLD", "0.90"))
        self.query_cache = QueryCacheService(
            cache_path="./data/chroma_cache",
            similarity_threshold=cache_threshold
        )

        # LLM ì œê³µì ì„ íƒ
        self.chat_provider = os.getenv("CHAT_PROVIDER", "solar")

        # Solar Pro ì´ˆê¸°í™”
        if self.chat_provider == "solar":
            self.solar_service = SolarService()
            self.chat_model = self.solar_service.chat_model
            logger.info(f"ì±„íŒ… ì œê³µì: Solar Pro ({self.chat_model})")
        else:
            # OpenAI ì´ˆê¸°í™”
            self.openai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.chat_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            logger.info(f"ì±„íŒ… ì œê³µì: OpenAI ({self.chat_model})")

        # ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ (ì„¸ì…˜ ìºì‹œ - ê°œì¸, 30ë¶„ TTL)
        # ì‹¤ì „ì—ì„œëŠ” Redisë‚˜ DB ì‚¬ìš©
        self.conversation_history = {}

        # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ ê´€ë¦¬ (ë³´ì•ˆ)
        self.session_expiry = {}  # {session_id: expiry_datetime}
        self.session_ttl_minutes = int(os.getenv("SESSION_TTL_MINUTES", "30"))  # ê¸°ë³¸ 30ë¶„

        logger.info(f"ChatbotService ì´ˆê¸°í™” ì™„ë£Œ (ì„¸ì…˜ TTL: {self.session_ttl_minutes}ë¶„)")


    def _clean_expired_sessions(self):
        """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ (ë³´ì•ˆ + ë©”ëª¨ë¦¬ ê´€ë¦¬)"""
        now = datetime.now()
        expired_sessions = [
            session_id for session_id, expiry in self.session_expiry.items()
            if expiry < now
        ]

        for session_id in expired_sessions:
            if session_id in self.conversation_history:
                del self.conversation_history[session_id]
            del self.session_expiry[session_id]
            logger.info(f"ë§Œë£Œëœ ì„¸ì…˜ ì‚­ì œ: {session_id}")


    def _update_session_expiry(self, session_id: str):
        """ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ ê°±ì‹ """
        self.session_expiry[session_id] = datetime.now() + timedelta(minutes=self.session_ttl_minutes)


    def _is_smartstore_question(self, query: str) -> bool:
        """
        ë„ë©”ì¸ í•„í„°ë§: ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸ (FSF í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ)

        3ë‹¨ê³„ í•„í„°ë§:
        1. ëª…í™•í•œ í‚¤ì›Œë“œ â†’ ì¦‰ì‹œ í†µê³¼ (ë¹ ë¦„, ë¬´ë£Œ)
        2. í™•ì¥ í‚¤ì›Œë“œ â†’ LLM ê²€ì¦ (ëŠë¦¼, ìœ ë£Œì§€ë§Œ ìºì‹±ë¨)
        3. ì™„ì „ ë¬´ê´€ â†’ ì°¨ë‹¨

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ True
        """
        query_lower = query.lower()

        # 1ë‹¨ê³„: ëª…í™•í•œ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ í‚¤ì›Œë“œ (100% í™•ì‹ )
        core_keywords = [
            "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´", "smartstore", "ë„¤ì´ë²„ìŠ¤í† ì–´", "ì…€ëŸ¬", "íŒë§¤ìì„¼í„°"
        ]
        if any(kw in query_lower for kw in core_keywords):
            logger.info(f"âœ… 1ë‹¨ê³„ í†µê³¼ (ëª…í™•í•œ í‚¤ì›Œë“œ)")
            return True

        # 2ë‹¨ê³„: ì „ììƒê±°ë˜ ê´€ë ¨ í‚¤ì›Œë“œ (ì• ë§¤í•¨ â†’ LLM ê²€ì¦)
        # ì´ í‚¤ì›Œë“œë“¤ì€ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì™¸ì—ë„ ì“°ì¼ ìˆ˜ ìˆìŒ
        commerce_keywords = [
            # íŒë§¤ ê´€ë ¨
            "íŒë§¤", "ìƒí’ˆ", "ì•„ì´í…œ", "ë¬¼ê±´", "ê°€ê²©", "í• ì¸", "ì¿ í°",
            # ì£¼ë¬¸/ë°°ì†¡ ê´€ë ¨
            "ì£¼ë¬¸", "ë°°ì†¡", "íƒë°°", "ë°œì†¡", "ì†¡ì¥", "í¬ì¥",
            # ê²°ì œ/ì •ì‚° ê´€ë ¨
            "ê²°ì œ", "ì •ì‚°", "ìˆ˜ìˆ˜ë£Œ", "ì…ê¸ˆ", "ëˆ", "ì„¸ê¸ˆ", "ê³„ì¢Œ",
            # í™˜ë¶ˆ/êµí™˜/ì·¨ì†Œ ê´€ë ¨
            "í™˜ë¶ˆ", "êµí™˜", "ë°˜í’ˆ", "ì·¨ì†Œ", "í´ë ˆì„", "as", "ë¶ˆëŸ‰",
            # ê³ ê°ì‘ëŒ€ ê´€ë ¨
            "ê³ ê°", "êµ¬ë§¤ì", "ë¬¸ì˜", "ë¦¬ë·°", "í›„ê¸°", "í‰ì ", "ìš•", "ì»´í”Œë ˆì¸",
            # ìŠ¤í† ì–´ ê´€ë¦¬ ê´€ë ¨
            "ê°€ì…", "ë“±ë¡", "ê°œì„¤", "ìš´ì˜", "ê´€ë¦¬", "ë…¸ì¶œ", "ê²€ìƒ‰", "ì¹´í…Œê³ ë¦¬",
            # ê¸°íƒ€
            "ì‡¼í•‘", "ìŠ¤í† ì–´", "ì‚¬ì§„", "ì´ë¯¸ì§€", "ì‚¬ì—…ì", "ëŒ€í‘œì"
        ]

        if any(kw in query for kw in commerce_keywords):
            logger.info(f"âš ï¸  2ë‹¨ê³„: í™•ì¥ í‚¤ì›Œë“œ ê°ì§€ â†’ LLM ê²€ì¦ í•„ìš”")
            # LLMìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ì¸ì§€ ìµœì¢… í™•ì¸
            return self._llm_verify_domain(query)

        # 3ë‹¨ê³„: ì™„ì „ ë¬´ê´€í•œ ì§ˆë¬¸ ì°¨ë‹¨
        logger.info(f"âŒ 3ë‹¨ê³„: í‚¤ì›Œë“œ ì—†ìŒ â†’ ì°¨ë‹¨")
        return False

    def _llm_verify_domain(self, query: str) -> bool:
        """
        LLMìœ¼ë¡œ ë„ë©”ì¸ ê²€ì¦ (2ë‹¨ê³„ í•„í„°ìš©)

        ë¹„ìš© ìµœì í™”:
        - gpt-4o-mini ì‚¬ìš© (ì €ë ´)
        - max_tokens=5 (YES/NOë§Œ)
        - ê²°ê³¼ëŠ” ì¿¼ë¦¬ ìºì‹œì— ì €ì¥ë¨ â†’ ê°™ì€ ì§ˆë¬¸ ì¬ì‚¬ìš© ì‹œ ë¬´ë£Œ!

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ì ê´€ë ¨ ì§ˆë¬¸ì´ë©´ True
        """
        try:
            # ê°„ë‹¨í•œ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì´ 'ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ì'ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ì ê´€ë ¨ ì£¼ì œ:
- ìŠ¤í† ì–´ ê°œì„¤/ê°€ì…/ë“±ë¡ (ì—°ë ¹, ìê²©, ì„œë¥˜ ë“±)
- ìƒí’ˆ ë“±ë¡/ê´€ë¦¬
- ì£¼ë¬¸/ë°°ì†¡ ì²˜ë¦¬
- ê²°ì œ/ì •ì‚°
- í™˜ë¶ˆ/êµí™˜/ì·¨ì†Œ ì²˜ë¦¬
- ê³ ê° ë¬¸ì˜/ë¦¬ë·° ê´€ë¦¬
- íŒë§¤ ì „ëµ/ë§ˆì¼€íŒ…
- íŒë§¤ì ìê²© ìš”ê±´ (ë¯¸ì„±ë…„ì, ì‚¬ì—…ì ë“±)

ì§ˆë¬¸: {query}

ìœ„ ì§ˆë¬¸ì´ ìœ„ ì£¼ì œ ì¤‘ í•˜ë‚˜ì™€ ê´€ë ¨ë˜ë©´ 'YES', ì™„ì „íˆ ë¬´ê´€í•˜ë©´ 'NO'ë§Œ ë‹µí•˜ì„¸ìš”."""

            # OpenAI ë˜ëŠ” Solar ì‚¬ìš© (ë‘˜ ë‹¤ ì €ë ´)
            if hasattr(self, 'openai'):
                response = self.openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=5,
                    temperature=0
                )
                result = response.choices[0].message.content.strip().upper()
            else:
                # Solar ì‚¬ìš© ì‹œ
                result = self.solar_service.generate_chat_response(
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0,
                    max_tokens=5
                ).strip().upper()

            is_related = "YES" in result
            logger.info(f"ğŸ¤– LLM ê²€ì¦ ê²°ê³¼: {'âœ… ê´€ë ¨ìˆìŒ' if is_related else 'âŒ ë¬´ê´€'}")
            return is_related

        except Exception as e:
            logger.error(f"LLM ê²€ì¦ ì‹¤íŒ¨: {e} â†’ ì•ˆì „í•˜ê²Œ True ë°˜í™˜")
            # ì—ëŸ¬ ì‹œ ì•ˆì „í•˜ê²Œ í†µê³¼ (False Negative ë°©ì§€)
            return True


    def _generate_system_prompt(self) -> str:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Solar Proìš©ìœ¼ë¡œ ìµœì í™”ëœ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸

        Returns:
            ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        """
        return """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì „ë¬¸ ìƒë‹´ AIì…ë‹ˆë‹¤.

ì—­í• :
- ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ìë“¤ì˜ FAQ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤
- ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
- ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤

ë‹µë³€ ê·œì¹™:
1. ì œê³µëœ FAQ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”
3. ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. í•„ìš”ì‹œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”
5. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”

ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°:
"ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”."""


    def _generate_follow_up_questions(
        self,
        query: str,
        answer: str,
        search_results: List[Dict[str, Any]]
    ) -> List[str]:
        """
        í›„ì† ì§ˆë¬¸ ìƒì„± (LLM ê¸°ë°˜ - í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ)

        ê²€ìƒ‰ ê²°ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ì§ˆë¬¸ 3ê°œ ìƒì„±

        Args:
            query: ì›ë˜ ì§ˆë¬¸
            answer: ìƒì„±ëœ ë‹µë³€
            search_results: ê²€ìƒ‰ ê²°ê³¼

        Returns:
            í›„ì† ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 3ê°œ)
        """
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        categories = []
        for result in search_results[:3]:
            if "metadata" in result and "category" in result["metadata"]:
                cat = result["metadata"]["category"]
                if cat != "ê¸°íƒ€":
                    categories.append(cat)

        if not categories:
            categories = ["ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì¼ë°˜"]

        categories_str = ", ".join(set(categories))

        # LLMìœ¼ë¡œ í›„ì† ì§ˆë¬¸ ìƒì„±
        prompt = f"""ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ FAQ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ ê´€ë ¨ ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}
ê´€ë ¨ ì¹´í…Œê³ ë¦¬: {categories_str}

ê·œì¹™:
1. ìœ„ ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ëœ ì‹¤ìš©ì ì¸ ì§ˆë¬¸
2. ì‚¬ìš©ìê°€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ê¶ê¸ˆí•´í•  ë‚´ìš©
3. ê°„ê²°í•˜ê³  ëª…í™•í•œ ì§ˆë¬¸ (15ì ì´ë‚´)

ì§ˆë¬¸ 3ê°œë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±í•˜ì„¸ìš”:"""

        try:
            if hasattr(self, 'openai'):
                response = self.openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=150,
                    temperature=0.7
                )
                result = response.choices[0].message.content.strip()
            else:
                result = self.solar_service.generate_chat_response(
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=150
                ).strip()

            # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬ í›„ ì •ì œ
            questions = [q.strip().lstrip('123.-â€¢') for q in result.split('\n') if q.strip()]
            return questions[:3]

        except Exception as e:
            logger.error(f"í›„ì† ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‹¤ë¥¸ ì§ˆë¬¸ ì¶”ì¶œ
            fallback = []
            for result in search_results[:3]:
                if "metadata" in result and "clean_question" in result["metadata"]:
                    fallback.append(result["metadata"]["clean_question"])

            # ê²€ìƒ‰ ê²°ê³¼ë„ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return fallback[:3] if fallback else []


    def _generate_contextual_questions(self, query: str, answer: str, search_results: List[Dict[str, Any]]) -> List[str]:
        """
        ë§¥ë½ ê¸°ë°˜ ì—­ì§ˆë¬¸ ìƒì„± (ì§ˆë¬¸ë§Œ! ë‹µë³€ì€ í´ë¦­ ì‹œ ìƒì„±)

        ë‹µë³€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ ì—­ì§ˆë¬¸ 2ê°œë¥¼ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            answer: ìƒì„±ëœ ë‹µë³€
            search_results: ê²€ìƒ‰ ê²°ê³¼

        Returns:
            ì—­ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 2ê°œ)
        """
        # LLMìœ¼ë¡œ ì—­ì§ˆë¬¸ë§Œ ìƒì„± (ë‹µë³€ì€ ë‚˜ì¤‘ì— í´ë¦­ ì‹œ!)
        prompt = f"""ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ FAQ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.
ë‹µë³€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìê°€ ë‹¤ìŒìœ¼ë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ ì—­ì§ˆë¬¸ 2ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}
ì±—ë´‡ ë‹µë³€: {answer}

ì—­ì§ˆë¬¸ ê·œì¹™:
1. ë‹µë³€ ë‚´ìš©ê³¼ ì§ì ‘ ì—°ê´€ëœ í›„ì† ì§ˆë¬¸
2. "~ì•ˆë‚´í•´ë“œë¦´ê¹Œìš”?", "~ì•Œë ¤ë“œë¦´ê¹Œìš”?", "~í•„ìš”í•˜ì‹ ê°€ìš”?" í˜•ì‹ ì‚¬ìš©
3. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì§ˆë¬¸ (20ì ì´ë‚´)

ì§ˆë¬¸ 2ê°œë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•´ì„œ ì‘ì„±í•˜ì„¸ìš”:"""

        try:
            if hasattr(self, 'openai'):
                response = self.openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=150,
                    temperature=0.7
                )
                result = response.choices[0].message.content.strip()
            else:
                result = self.solar_service.generate_chat_response(
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.7,
                    max_tokens=150
                ).strip()

            # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬
            questions = [q.strip() for q in result.split('\n') if q.strip()]

            # ë²ˆí˜¸ ì œê±° (1. 2. - * ë“±)
            import re
            questions = [re.sub(r'^[\d\-\*\.\)]+\s*', '', q).strip() for q in questions]

            return questions[:2]

        except Exception as e:
            logger.error(f"ì—­ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return []


    def answer_contextual_question(
        self,
        contextual_question: str,
        original_query: str,
        original_answer: str,
        session_id: str = "default"
    ) -> str:
        """
        ì—­ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ê¸°ì¡´ main answerì—ì„œ ì¶”ì¶œ!)

        ì‚¬ìš©ìê°€ ì—­ì§ˆë¬¸ì„ í´ë¦­í•˜ë©´, ì´ë¯¸ ìƒì„±ëœ main answerì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€

        Args:
            contextual_question: ì—­ì§ˆë¬¸
            original_query: ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸
            original_answer: ì›ë˜ ë‹µë³€ (ì—¬ê¸°ì„œ ì •ë³´ ì¶”ì¶œ!)
            session_id: ì„¸ì…˜ ID

        Returns:
            ì—­ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€
        """
        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        cached_result = self.query_cache.search_similar_cache(contextual_question)
        if cached_result:
            logger.info(f"ğŸ¯ ì—­ì§ˆë¬¸ ìºì‹œ íˆíŠ¸: {contextual_question[:30]}...")
            return cached_result["answer"]

        # ìºì‹œ ë¯¸ìŠ¤: main answerì—ì„œ ì •ë³´ ì¶”ì¶œ
        prompt = f"""ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ FAQ ì±—ë´‡ì…ë‹ˆë‹¤.

ì›ë˜ ì‚¬ìš©ì ì§ˆë¬¸: {original_query}
ì›ë˜ ë‹µë³€:
{original_answer}

ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ê¶ê¸ˆí•´í•˜ëŠ” ì§ˆë¬¸: {contextual_question}

**ìœ„ ì›ë˜ ë‹µë³€ì— í¬í•¨ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ** ì‚¬ìš©ìì˜ ì¶”ê°€ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. (100ì ì´ë‚´)
ìƒˆë¡œìš´ ì •ë³´ë¥¼ ë§Œë“¤ì§€ ë§ê³ , ì´ë¯¸ ì œê³µí•œ ë‹µë³€ì—ì„œ ê´€ë ¨ ë¶€ë¶„ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”."""

        try:
            # LLM í˜¸ì¶œ
            if hasattr(self, 'openai'):
                response = self.openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=200,
                    temperature=0.5
                )
                answer = response.choices[0].message.content.strip()
            else:
                answer = self.solar_service.generate_chat_response(
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.5,
                    max_tokens=200
                ).strip()

            # ìºì‹œì— ì €ì¥ (ë‹¤ìŒ ì‚¬ìš©ìë¥¼ ìœ„í•´!)
            self.query_cache.save_cache(
                query=contextual_question,
                answer=answer,
                follow_up_questions=[],
                contextual_questions=[],
                sources=[]
            )
            logger.info(f"ğŸ’¾ ì—­ì§ˆë¬¸ ë‹µë³€ ìºì‹œ ì €ì¥: {contextual_question[:30]}...")

            return answer

        except Exception as e:
            logger.error(f"ì—­ì§ˆë¬¸ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


    def chat(
        self,
        query: str,
        session_id: str = "default",
        use_hybrid: bool = True,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        ì¼ë°˜ ì±„íŒ… (ë¹„ìŠ¤íŠ¸ë¦¬ë°)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID (ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ìš©)
            use_hybrid: Hybrid RAG ì‚¬ìš© ì—¬ë¶€
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Returns:
            {
                "answer": "ë‹µë³€",
                "follow_up_questions": ["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3"],
                "sources": [ê²€ìƒ‰ëœ ë¬¸ì„œë“¤],
                "is_smartstore_related": True/False,
                "cached": True/False  # ìºì‹œ íˆíŠ¸ ì—¬ë¶€
            }
        """
        # 0. ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ (ë³´ì•ˆ)
        self._clean_expired_sessions()

        # 1. ë„ë©”ì¸ í•„í„°ë§
        is_smartstore = self._is_smartstore_question(query)
        if not is_smartstore:
            # RAGì—ì„œ ì¼ë°˜ì ì¸ FAQ ì§ˆë¬¸ 3ê°œ ì¶”ì¶œ (í•˜ë“œì½”ë”© ì œê±°)
            general_faqs = self.rag_service.semantic_search("ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì…", top_k=3)
            suggested_questions = [
                faq["metadata"]["clean_question"] for faq in general_faqs
            ] if general_faqs else []

            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?",
                "follow_up_questions": suggested_questions,
                "contextual_questions": [],
                "sources": [],
                "is_smartstore_related": False,
                "cached": False
            }

        # 2. ì¿¼ë¦¬ ìºì‹œ í™•ì¸ (ê¸€ë¡œë²Œ - 90% ë¹„ìš© ì ˆê°!)
        cached_result = self.query_cache.search_similar_cache(query)
        if cached_result:
            logger.info(f"ğŸ¯ ìºì‹œì—ì„œ ë‹µë³€ ë°˜í™˜! ìœ ì‚¬ë„: {cached_result['similarity']:.2%}")
            return {
                "answer": cached_result["answer"],
                "follow_up_questions": cached_result["follow_up_questions"],
                "contextual_questions": cached_result.get("contextual_questions", []),  # ì—­ì§ˆë¬¸ (ìºì‹œì— ìˆìœ¼ë©´)
                "sources": cached_result["sources"],
                "is_smartstore_related": True,
                "cached": True,
                "cache_similarity": cached_result["similarity"],
                "original_query": cached_result["original_query"]
            }

        # 3. RAG ê²€ìƒ‰ (ìºì‹œ ë¯¸ìŠ¤ ì‹œì—ë§Œ!)
        if use_hybrid:
            search_results = self.rag_service.hybrid_search(query, top_k=top_k)
        else:
            search_results = self.rag_service.semantic_search(query, top_k=top_k)

        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë‹µë³€
        if not search_results:
            # ì¼ë°˜ FAQì—ì„œ ì¶”ì²œ ì§ˆë¬¸ ì¶”ì¶œ (í•˜ë“œì½”ë”© ì œê±°)
            fallback_faqs = self.rag_service.semantic_search("ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´", top_k=3)
            fallback_questions = [
                faq["metadata"]["clean_question"] for faq in fallback_faqs
            ] if fallback_faqs else []

            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?",
                "follow_up_questions": fallback_questions,
                "contextual_questions": [],
                "sources": [],
                "is_smartstore_related": True,
                "cached": False
            }

        # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[ë¬¸ì„œ {i+1}] (ì¹´í…Œê³ ë¦¬: {doc['metadata']['category']})\n{doc['document']}"
            for i, doc in enumerate(search_results[:3])
        ])

        # 5. ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []

        history = self.conversation_history[session_id]

        # 6. ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self._generate_system_prompt()}
        ]

        # ìµœê·¼ ëŒ€í™” 3í„´ë§Œ í¬í•¨ (ë©”ëª¨ë¦¬ ì ˆì•½)
        for msg in history[-6:]:
            messages.append(msg)

        # í˜„ì¬ ì§ˆë¬¸
        user_message = f"""ê´€ë ¨ ë¬¸ì„œ:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        messages.append({"role": "user", "content": user_message})

        # 7. LLM ë‹µë³€ ìƒì„± (ìºì‹œ ë¯¸ìŠ¤ ì‹œì—ë§Œ!)
        if self.chat_provider == "solar":
            answer = self.solar_service.generate_chat_response(
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
        else:
            response = self.openai.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            answer = response.choices[0].message.content

        # 8. í›„ì† ì§ˆë¬¸ ìƒì„± (LLM ê¸°ë°˜ - ì¹´í…Œê³ ë¦¬ ì°¸ê³ )
        follow_ups = self._generate_follow_up_questions(query, answer, search_results)

        # 9. ë§¥ë½ ê¸°ë°˜ ì—­ì§ˆë¬¸ + ë‹µë³€ ìƒì„± (LLM ê¸°ë°˜ - ë‹µë³€ ë‚´ìš© ì°¸ê³ )
        contextual_questions = self._generate_contextual_questions(query, answer, search_results)

        # 10. ì°¸ê³  ë¬¸ì„œ ì •ë¦¬
        sources = [
            {
                "category": doc["metadata"]["category"],
                "question": doc["metadata"]["clean_question"],
                "similarity": doc.get("similarity", doc.get("score", 0))
            }
            for doc in search_results[:3]
        ]

        # 11. ì¿¼ë¦¬ ìºì‹œì— ì €ì¥ (ë‹¤ìŒ ì‚¬ìš©ìë¥¼ ìœ„í•´!)
        self.query_cache.save_cache(
            query=query,
            answer=answer,
            follow_up_questions=follow_ups,
            contextual_questions=contextual_questions,
            sources=sources
        )

        # 11. ëŒ€í™” ê¸°ë¡ ì €ì¥ (ì„¸ì…˜ ìºì‹œ)
        self.conversation_history[session_id].append(
            {"role": "user", "content": query}
        )
        self.conversation_history[session_id].append(
            {"role": "assistant", "content": answer}
        )

        # ìµœê·¼ 10í„´ë§Œ ìœ ì§€
        if len(self.conversation_history[session_id]) > 20:
            self.conversation_history[session_id] = self.conversation_history[session_id][-20:]

        # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ ê°±ì‹  (ë³´ì•ˆ)
        self._update_session_expiry(session_id)

        # 12. ê²°ê³¼ ë°˜í™˜
        return {
            "answer": answer,
            "follow_up_questions": follow_ups,
            "contextual_questions": contextual_questions,  # ì—­ì§ˆë¬¸ ì¶”ê°€!
            "sources": sources,
            "is_smartstore_related": True,
            "cached": False  # ìƒˆë¡œ ìƒì„±í•œ ë‹µë³€
        }


    async def stream_chat(
        self,
        query: str,
        session_id: str = "default",
        use_hybrid: bool = True,
        top_k: int = 5
    ) -> AsyncIterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID
            use_hybrid: Hybrid RAG ì‚¬ìš© ì—¬ë¶€
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Yields:
            ë‹µë³€ ì²­í¬ (ë¬¸ìì—´)
        """
        # 1. ë„ë©”ì¸ í•„í„°ë§
        is_smartstore = self._is_smartstore_question(query)
        if not is_smartstore:
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
            return

        # 2. RAG ê²€ìƒ‰
        if use_hybrid:
            search_results = self.rag_service.hybrid_search(query, top_k=top_k)
        else:
            search_results = self.rag_service.semantic_search(query, top_k=top_k)

        if not search_results:
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
            return

        # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[ë¬¸ì„œ {i+1}] (ì¹´í…Œê³ ë¦¬: {doc['metadata']['category']})\n{doc['document']}"
            for i, doc in enumerate(search_results[:3])
        ])

        # 4. ëŒ€í™” ê¸°ë¡
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []

        history = self.conversation_history[session_id]

        # 5. ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self._generate_system_prompt()}
        ]

        for msg in history[-6:]:
            messages.append(msg)

        user_message = f"""ê´€ë ¨ ë¬¸ì„œ:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        messages.append({"role": "user", "content": user_message})

        # 6. ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
        full_answer = ""

        if self.chat_provider == "solar":
            async for chunk in self.solar_service.async_stream_chat_response(
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            ):
                full_answer += chunk
                yield chunk
        else:
            stream = self.openai.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_answer += content
                    yield content

        # 7. ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_history[session_id].append(
            {"role": "user", "content": query}
        )
        self.conversation_history[session_id].append(
            {"role": "assistant", "content": full_answer}
        )

        if len(self.conversation_history[session_id]) > 20:
            self.conversation_history[session_id] = self.conversation_history[session_id][-20:]


    def get_conversation_history(self, session_id: str = "default") -> List[Dict[str, str]]:
        """
        ëŒ€í™” ê¸°ë¡ ì¡°íšŒ

        Args:
            session_id: ì„¸ì…˜ ID

        Returns:
            ëŒ€í™” ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
        """
        return self.conversation_history.get(session_id, [])


    def clear_conversation_history(self, session_id: str = "default") -> None:
        """
        ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”

        Args:
            session_id: ì„¸ì…˜ ID
        """
        if session_id in self.conversation_history:
            del self.conversation_history[session_id]
            logger.info(f"ì„¸ì…˜ {session_id} ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()

    logging.basicConfig(level=logging.INFO)

    # ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    chatbot = ChatbotService()

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì…ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
        "ìƒí’ˆ ë“±ë¡ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"  # ë„ë©”ì¸ ì™¸ ì§ˆë¬¸
    ]

    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"ì§ˆë¬¸: {query}")
        print(f"{'='*60}")

        result = chatbot.chat(query)

        print(f"\në‹µë³€:\n{result['answer']}\n")
        print(f"ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨: {result['is_smartstore_related']}")
        print(f"\ní›„ì† ì§ˆë¬¸:")
        for i, fq in enumerate(result['follow_up_questions'], 1):
            print(f"  {i}. {fq}")

        if result['sources']:
            print(f"\nì°¸ê³  ë¬¸ì„œ:")
            for i, src in enumerate(result['sources'], 1):
                print(f"  {i}. [{src['category']}] {src['question'][:50]}...")
