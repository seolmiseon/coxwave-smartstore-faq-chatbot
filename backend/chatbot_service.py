# FAQ ì±—ë´‡ ì„œë¹„ìŠ¤ (OpenAI + Solar Pro í•˜ì´ë¸Œë¦¬ë“œ)
# ì½•ìŠ¤ì›¨ì´ë¸Œ ê³¼ì œ ì „í˜•

from rag_service import RAGService
from solar_service import SolarService
from cache_service import QueryCacheService
from openai import OpenAI
import os
import logging
from typing import List, Dict, Any, Optional, AsyncIterator
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


class ChatbotService:
    """
    FAQ ì±—ë´‡ ë©”ì¸ ì„œë¹„ìŠ¤

    ì°¨ë³„í™” í¬ì¸íŠ¸:
    1. OpenAI (ì„ë² ë”©) + Solar Pro (ì±„íŒ…) í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ
    2. Hybrid RAG (Semantic + Keyword + RRF)
    3. ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
    4. í›„ì† ì§ˆë¬¸ ìƒì„±
    5. ë„ë©”ì¸ í•„í„°ë§ (ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì „ìš©)
    """

    def __init__(self):
        """ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.rag_service = RAGService(
            persist_directory=os.getenv("CHROMA_PERSIST_DIRECTORY", "./chroma_db"),
            collection_name="smartstore_faq"
        )

        # ì¿¼ë¦¬ ìºì‹œ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ê¸€ë¡œë²Œ ìºì‹± - 90% ë¹„ìš© ì ˆê°)
        cache_threshold = float(os.getenv("CACHE_SIMILARITY_THRESHOLD", "0.90"))
        self.query_cache = QueryCacheService(
            cache_path="./data/chroma_cache",
            similarity_threshold=cache_threshold
        )

        # LLM ì œê³µì ì„ íƒ
        self.chat_provider = os.getenv("CHAT_PROVIDER", "solar")

        # Solar Pro ì´ˆê¸°í™”
        if self.chat_provider == "solar":
            self.solar_service = SolarService()
            self.chat_model = self.solar_service.chat_model
            logger.info(f"ì±„íŒ… ì œê³µì: Solar Pro ({self.chat_model})")
        else:
            # OpenAI ì´ˆê¸°í™”
            self.openai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.chat_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            logger.info(f"ì±„íŒ… ì œê³µì: OpenAI ({self.chat_model})")

        # ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ (ì„¸ì…˜ ìºì‹œ - ê°œì¸, 30ë¶„ TTL)
        # ì‹¤ì „ì—ì„œëŠ” Redisë‚˜ DB ì‚¬ìš©
        self.conversation_history = {}

        # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ ê´€ë¦¬ (ë³´ì•ˆ)
        self.session_expiry = {}  # {session_id: expiry_datetime}
        self.session_ttl_minutes = int(os.getenv("SESSION_TTL_MINUTES", "30"))  # ê¸°ë³¸ 30ë¶„

        logger.info(f"ChatbotService ì´ˆê¸°í™” ì™„ë£Œ (ì„¸ì…˜ TTL: {self.session_ttl_minutes}ë¶„)")


    def _clean_expired_sessions(self):
        """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ (ë³´ì•ˆ + ë©”ëª¨ë¦¬ ê´€ë¦¬)"""
        now = datetime.now()
        expired_sessions = [
            session_id for session_id, expiry in self.session_expiry.items()
            if expiry < now
        ]

        for session_id in expired_sessions:
            if session_id in self.conversation_history:
                del self.conversation_history[session_id]
            del self.session_expiry[session_id]
            logger.info(f"ë§Œë£Œëœ ì„¸ì…˜ ì‚­ì œ: {session_id}")


    def _update_session_expiry(self, session_id: str):
        """ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ ê°±ì‹ """
        self.session_expiry[session_id] = datetime.now() + timedelta(minutes=self.session_ttl_minutes)


    def _is_smartstore_question(self, query: str) -> bool:
        """
        ë„ë©”ì¸ í•„í„°ë§: ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸ (FSF í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ)

        3ë‹¨ê³„ í•„í„°ë§:
        1. ëª…í™•í•œ í‚¤ì›Œë“œ â†’ ì¦‰ì‹œ í†µê³¼ (ë¹ ë¦„, ë¬´ë£Œ)
        2. í™•ì¥ í‚¤ì›Œë“œ â†’ LLM ê²€ì¦ (ëŠë¦¼, ìœ ë£Œì§€ë§Œ ìºì‹±ë¨)
        3. ì™„ì „ ë¬´ê´€ â†’ ì°¨ë‹¨

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ True
        """
        query_lower = query.lower()

        # 1ë‹¨ê³„: ëª…í™•í•œ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ í‚¤ì›Œë“œ (100% í™•ì‹ )
        core_keywords = [
            "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´", "smartstore", "ë„¤ì´ë²„ìŠ¤í† ì–´", "ì…€ëŸ¬", "íŒë§¤ìì„¼í„°"
        ]
        if any(kw in query_lower for kw in core_keywords):
            logger.info(f"âœ… 1ë‹¨ê³„ í†µê³¼ (ëª…í™•í•œ í‚¤ì›Œë“œ)")
            return True

        # 2ë‹¨ê³„: ì „ììƒê±°ë˜ ê´€ë ¨ í‚¤ì›Œë“œ (ì• ë§¤í•¨ â†’ LLM ê²€ì¦)
        # ì´ í‚¤ì›Œë“œë“¤ì€ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì™¸ì—ë„ ì“°ì¼ ìˆ˜ ìˆìŒ
        commerce_keywords = [
            # íŒë§¤ ê´€ë ¨
            "íŒë§¤", "ìƒí’ˆ", "ì•„ì´í…œ", "ë¬¼ê±´", "ê°€ê²©", "í• ì¸", "ì¿ í°",
            # ì£¼ë¬¸/ë°°ì†¡ ê´€ë ¨
            "ì£¼ë¬¸", "ë°°ì†¡", "íƒë°°", "ë°œì†¡", "ì†¡ì¥", "í¬ì¥",
            # ê²°ì œ/ì •ì‚° ê´€ë ¨
            "ê²°ì œ", "ì •ì‚°", "ìˆ˜ìˆ˜ë£Œ", "ì…ê¸ˆ", "ëˆ", "ì„¸ê¸ˆ", "ê³„ì¢Œ",
            # í™˜ë¶ˆ/êµí™˜/ì·¨ì†Œ ê´€ë ¨
            "í™˜ë¶ˆ", "êµí™˜", "ë°˜í’ˆ", "ì·¨ì†Œ", "í´ë ˆì„", "as", "ë¶ˆëŸ‰",
            # ê³ ê°ì‘ëŒ€ ê´€ë ¨
            "ê³ ê°", "êµ¬ë§¤ì", "ë¬¸ì˜", "ë¦¬ë·°", "í›„ê¸°", "í‰ì ", "ìš•", "ì»´í”Œë ˆì¸",
            # ìŠ¤í† ì–´ ê´€ë¦¬ ê´€ë ¨
            "ê°€ì…", "ë“±ë¡", "ê°œì„¤", "ìš´ì˜", "ê´€ë¦¬", "ë…¸ì¶œ", "ê²€ìƒ‰", "ì¹´í…Œê³ ë¦¬",
            # ê¸°íƒ€
            "ì‡¼í•‘", "ìŠ¤í† ì–´", "ì‚¬ì§„", "ì´ë¯¸ì§€", "ì‚¬ì—…ì", "ëŒ€í‘œì"
        ]

        if any(kw in query for kw in commerce_keywords):
            logger.info(f"âš ï¸  2ë‹¨ê³„: í™•ì¥ í‚¤ì›Œë“œ ê°ì§€ â†’ LLM ê²€ì¦ í•„ìš”")
            # LLMìœ¼ë¡œ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ì¸ì§€ ìµœì¢… í™•ì¸
            return self._llm_verify_domain(query)

        # 3ë‹¨ê³„: ì™„ì „ ë¬´ê´€í•œ ì§ˆë¬¸ ì°¨ë‹¨
        logger.info(f"âŒ 3ë‹¨ê³„: í‚¤ì›Œë“œ ì—†ìŒ â†’ ì°¨ë‹¨")
        return False

    def _llm_verify_domain(self, query: str) -> bool:
        """
        LLMìœ¼ë¡œ ë„ë©”ì¸ ê²€ì¦ (2ë‹¨ê³„ í•„í„°ìš©)

        ë¹„ìš© ìµœì í™”:
        - gpt-4o-mini ì‚¬ìš© (ì €ë ´)
        - max_tokens=5 (YES/NOë§Œ)
        - ê²°ê³¼ëŠ” ì¿¼ë¦¬ ìºì‹œì— ì €ì¥ë¨ â†’ ê°™ì€ ì§ˆë¬¸ ì¬ì‚¬ìš© ì‹œ ë¬´ë£Œ!

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ì ê´€ë ¨ ì§ˆë¬¸ì´ë©´ True
        """
        try:
            # ê°„ë‹¨í•œ ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸
            prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì´ 'ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ì'ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ì ê´€ë ¨ ì£¼ì œ:
- ìŠ¤í† ì–´ ê°œì„¤/ê°€ì…
- ìƒí’ˆ ë“±ë¡/ê´€ë¦¬
- ì£¼ë¬¸/ë°°ì†¡ ì²˜ë¦¬
- ê²°ì œ/ì •ì‚°
- í™˜ë¶ˆ/êµí™˜/ì·¨ì†Œ ì²˜ë¦¬
- ê³ ê° ë¬¸ì˜/ë¦¬ë·° ê´€ë¦¬
- íŒë§¤ ì „ëµ/ë§ˆì¼€íŒ…

ì§ˆë¬¸: {query}

ìœ„ ì§ˆë¬¸ì´ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ìì™€ ê´€ë ¨ë˜ë©´ 'YES', ì•„ë‹ˆë©´ 'NO'ë§Œ ë‹µí•˜ì„¸ìš”."""

            # OpenAI ë˜ëŠ” Solar ì‚¬ìš© (ë‘˜ ë‹¤ ì €ë ´)
            if hasattr(self, 'openai'):
                response = self.openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=5,
                    temperature=0
                )
                result = response.choices[0].message.content.strip().upper()
            else:
                # Solar ì‚¬ìš© ì‹œ
                result = self.solar_service.generate_chat_response(
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0,
                    max_tokens=5
                ).strip().upper()

            is_related = "YES" in result
            logger.info(f"ğŸ¤– LLM ê²€ì¦ ê²°ê³¼: {'âœ… ê´€ë ¨ìˆìŒ' if is_related else 'âŒ ë¬´ê´€'}")
            return is_related

        except Exception as e:
            logger.error(f"LLM ê²€ì¦ ì‹¤íŒ¨: {e} â†’ ì•ˆì „í•˜ê²Œ True ë°˜í™˜")
            # ì—ëŸ¬ ì‹œ ì•ˆì „í•˜ê²Œ í†µê³¼ (False Negative ë°©ì§€)
            return True


    def _generate_system_prompt(self) -> str:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Solar Proìš©ìœ¼ë¡œ ìµœì í™”ëœ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸

        Returns:
            ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        """
        return """ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ì „ë¬¸ ìƒë‹´ AIì…ë‹ˆë‹¤.

ì—­í• :
- ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ íŒë§¤ìë“¤ì˜ FAQ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•©ë‹ˆë‹¤
- ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë©°, ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
- ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤

ë‹µë³€ ê·œì¹™:
1. ì œê³µëœ FAQ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ì•Œë ¤ì£¼ì„¸ìš”
3. ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•˜ì„¸ìš”
4. í•„ìš”ì‹œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”
5. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”

ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°:
"ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”."""


    def _generate_follow_up_questions(
        self,
        query: str,
        answer: str,
        search_results: List[Dict[str, Any]]
    ) -> List[str]:
        """
        í›„ì† ì§ˆë¬¸ ìƒì„±

        ì‚¬ìš©ìê°€ ì¶”ê°€ë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ ì§ˆë¬¸ 3ê°œ ìƒì„±

        Args:
            query: ì›ë˜ ì§ˆë¬¸
            answer: ìƒì„±ëœ ë‹µë³€
            search_results: ê²€ìƒ‰ ê²°ê³¼

        Returns:
            í›„ì† ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 3ê°œ)
        """
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        categories = set()
        for result in search_results[:5]:
            if "metadata" in result and "category" in result["metadata"]:
                cat = result["metadata"]["category"]
                if cat != "ê¸°íƒ€":
                    categories.add(cat)

        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í›„ì† ì§ˆë¬¸ ìƒì„±
        follow_ups = []

        category_questions = {
            "ê°€ì…ì ˆì°¨": "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì… ì‹œ í•„ìš”í•œ ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ê°€ì…ì„œë¥˜": "ì‚¬ì—…ìë“±ë¡ì¦ì´ ì—†ì–´ë„ ê°€ì…í•  ìˆ˜ ìˆë‚˜ìš”?",
            "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´": "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ì™€ ìŠ¤ë§ˆíŠ¸í”Œë ˆì´ìŠ¤ì˜ ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ìƒí’ˆë“±ë¡": "ìƒí’ˆ ë“±ë¡ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
            "ì£¼ë¬¸ê´€ë¦¬": "ì£¼ë¬¸ ì·¨ì†ŒëŠ” ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ë‚˜ìš”?",
            "ì •ì‚°": "ì •ì‚°ì€ ì–¸ì œ ì´ë£¨ì–´ì§€ë‚˜ìš”?",
            "ë°°ì†¡": "ë°°ì†¡ë¹„ëŠ” ì–´ë–»ê²Œ ì„¤ì •í•˜ë‚˜ìš”?",
        }

        for cat in list(categories)[:3]:
            if cat in category_questions:
                follow_ups.append(category_questions[cat])

        # ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ì§ˆë¬¸ ì¶”ê°€
        if len(follow_ups) < 3:
            general_questions = [
                "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ìˆ˜ìˆ˜ë£ŒëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
                "ìƒí’ˆ ë“±ë¡ì€ ëª‡ ê°œê¹Œì§€ ê°€ëŠ¥í•œê°€ìš”?",
                "ê³ ê° ë¬¸ì˜ëŠ” ì–´ë–»ê²Œ ê´€ë¦¬í•˜ë‚˜ìš”?"
            ]
            follow_ups.extend(general_questions[:3 - len(follow_ups)])

        return follow_ups[:3]


    def chat(
        self,
        query: str,
        session_id: str = "default",
        use_hybrid: bool = True,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        ì¼ë°˜ ì±„íŒ… (ë¹„ìŠ¤íŠ¸ë¦¬ë°)

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID (ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ìš©)
            use_hybrid: Hybrid RAG ì‚¬ìš© ì—¬ë¶€
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Returns:
            {
                "answer": "ë‹µë³€",
                "follow_up_questions": ["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3"],
                "sources": [ê²€ìƒ‰ëœ ë¬¸ì„œë“¤],
                "is_smartstore_related": True/False,
                "cached": True/False  # ìºì‹œ íˆíŠ¸ ì—¬ë¶€
            }
        """
        # 0. ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ (ë³´ì•ˆ)
        self._clean_expired_sessions()

        # 1. ë„ë©”ì¸ í•„í„°ë§
        is_smartstore = self._is_smartstore_question(query)
        if not is_smartstore:
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?",
                "follow_up_questions": [
                    "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì…ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
                    "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ìˆ˜ìˆ˜ë£ŒëŠ” ì–¼ë§ˆì¸ê°€ìš”?",
                    "ìƒí’ˆ ë“±ë¡ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
                ],
                "sources": [],
                "is_smartstore_related": False,
                "cached": False
            }

        # 2. ì¿¼ë¦¬ ìºì‹œ í™•ì¸ (ê¸€ë¡œë²Œ - 90% ë¹„ìš© ì ˆê°!)
        cached_result = self.query_cache.search_similar_cache(query)
        if cached_result:
            logger.info(f"ğŸ¯ ìºì‹œì—ì„œ ë‹µë³€ ë°˜í™˜! ìœ ì‚¬ë„: {cached_result['similarity']:.2%}")
            return {
                "answer": cached_result["answer"],
                "follow_up_questions": cached_result["follow_up_questions"],
                "sources": cached_result["sources"],
                "is_smartstore_related": True,
                "cached": True,
                "cache_similarity": cached_result["similarity"],
                "original_query": cached_result["original_query"]
            }

        # 3. RAG ê²€ìƒ‰ (ìºì‹œ ë¯¸ìŠ¤ ì‹œì—ë§Œ!)
        if use_hybrid:
            search_results = self.rag_service.hybrid_search(query, top_k=top_k)
        else:
            search_results = self.rag_service.semantic_search(query, top_k=top_k)

        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë‹µë³€
        if not search_results:
            return {
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?",
                "follow_up_questions": [
                    "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì…ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
                    "ìƒí’ˆ ë“±ë¡ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                    "ì£¼ë¬¸ ê´€ë¦¬ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
                ],
                "sources": [],
                "is_smartstore_related": True,
                "cached": False
            }

        # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[ë¬¸ì„œ {i+1}] (ì¹´í…Œê³ ë¦¬: {doc['metadata']['category']})\n{doc['document']}"
            for i, doc in enumerate(search_results[:3])
        ])

        # 5. ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []

        history = self.conversation_history[session_id]

        # 6. ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self._generate_system_prompt()}
        ]

        # ìµœê·¼ ëŒ€í™” 3í„´ë§Œ í¬í•¨ (ë©”ëª¨ë¦¬ ì ˆì•½)
        for msg in history[-6:]:
            messages.append(msg)

        # í˜„ì¬ ì§ˆë¬¸
        user_message = f"""ê´€ë ¨ ë¬¸ì„œ:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        messages.append({"role": "user", "content": user_message})

        # 7. LLM ë‹µë³€ ìƒì„± (ìºì‹œ ë¯¸ìŠ¤ ì‹œì—ë§Œ!)
        if self.chat_provider == "solar":
            answer = self.solar_service.generate_chat_response(
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
        else:
            response = self.openai.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            )
            answer = response.choices[0].message.content

        # 8. í›„ì† ì§ˆë¬¸ ìƒì„±
        follow_ups = self._generate_follow_up_questions(query, answer, search_results)

        # 9. ì°¸ê³  ë¬¸ì„œ ì •ë¦¬
        sources = [
            {
                "category": doc["metadata"]["category"],
                "question": doc["metadata"]["clean_question"],
                "similarity": doc.get("similarity", doc.get("score", 0))
            }
            for doc in search_results[:3]
        ]

        # 10. ì¿¼ë¦¬ ìºì‹œì— ì €ì¥ (ë‹¤ìŒ ì‚¬ìš©ìë¥¼ ìœ„í•´!)
        self.query_cache.save_cache(
            query=query,
            answer=answer,
            follow_up_questions=follow_ups,
            sources=sources
        )

        # 11. ëŒ€í™” ê¸°ë¡ ì €ì¥ (ì„¸ì…˜ ìºì‹œ)
        self.conversation_history[session_id].append(
            {"role": "user", "content": query}
        )
        self.conversation_history[session_id].append(
            {"role": "assistant", "content": answer}
        )

        # ìµœê·¼ 10í„´ë§Œ ìœ ì§€
        if len(self.conversation_history[session_id]) > 20:
            self.conversation_history[session_id] = self.conversation_history[session_id][-20:]

        # ì„¸ì…˜ ë§Œë£Œ ì‹œê°„ ê°±ì‹  (ë³´ì•ˆ)
        self._update_session_expiry(session_id)

        # 12. ê²°ê³¼ ë°˜í™˜
        return {
            "answer": answer,
            "follow_up_questions": follow_ups,
            "sources": sources,
            "is_smartstore_related": True,
            "cached": False  # ìƒˆë¡œ ìƒì„±í•œ ë‹µë³€
        }


    async def stream_chat(
        self,
        query: str,
        session_id: str = "default",
        use_hybrid: bool = True,
        top_k: int = 5
    ) -> AsyncIterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID
            use_hybrid: Hybrid RAG ì‚¬ìš© ì—¬ë¶€
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜

        Yields:
            ë‹µë³€ ì²­í¬ (ë¬¸ìì—´)
        """
        # 1. ë„ë©”ì¸ í•„í„°ë§
        is_smartstore = self._is_smartstore_question(query)
        if not is_smartstore:
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
            return

        # 2. RAG ê²€ìƒ‰
        if use_hybrid:
            search_results = self.rag_service.hybrid_search(query, top_k=top_k)
        else:
            search_results = self.rag_service.semantic_search(query, top_k=top_k)

        if not search_results:
            yield "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
            return

        # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[ë¬¸ì„œ {i+1}] (ì¹´í…Œê³ ë¦¬: {doc['metadata']['category']})\n{doc['document']}"
            for i, doc in enumerate(search_results[:3])
        ])

        # 4. ëŒ€í™” ê¸°ë¡
        if session_id not in self.conversation_history:
            self.conversation_history[session_id] = []

        history = self.conversation_history[session_id]

        # 5. ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": self._generate_system_prompt()}
        ]

        for msg in history[-6:]:
            messages.append(msg)

        user_message = f"""ê´€ë ¨ ë¬¸ì„œ:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

        messages.append({"role": "user", "content": user_message})

        # 6. ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
        full_answer = ""

        if self.chat_provider == "solar":
            async for chunk in self.solar_service.async_stream_chat_response(
                messages=messages,
                temperature=0.7,
                max_tokens=1000
            ):
                full_answer += chunk
                yield chunk
        else:
            stream = self.openai.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                stream=True
            )

            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_answer += content
                    yield content

        # 7. ëŒ€í™” ê¸°ë¡ ì €ì¥
        self.conversation_history[session_id].append(
            {"role": "user", "content": query}
        )
        self.conversation_history[session_id].append(
            {"role": "assistant", "content": full_answer}
        )

        if len(self.conversation_history[session_id]) > 20:
            self.conversation_history[session_id] = self.conversation_history[session_id][-20:]


    def get_conversation_history(self, session_id: str = "default") -> List[Dict[str, str]]:
        """
        ëŒ€í™” ê¸°ë¡ ì¡°íšŒ

        Args:
            session_id: ì„¸ì…˜ ID

        Returns:
            ëŒ€í™” ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
        """
        return self.conversation_history.get(session_id, [])


    def clear_conversation_history(self, session_id: str = "default") -> None:
        """
        ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”

        Args:
            session_id: ì„¸ì…˜ ID
        """
        if session_id in self.conversation_history:
            del self.conversation_history[session_id]
            logger.info(f"ì„¸ì…˜ {session_id} ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()

    logging.basicConfig(level=logging.INFO)

    # ì±—ë´‡ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    chatbot = ChatbotService()

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_queries = [
        "ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê°€ì…ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
        "ìƒí’ˆ ë“±ë¡ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?"  # ë„ë©”ì¸ ì™¸ ì§ˆë¬¸
    ]

    for query in test_queries:
        print(f"\n{'='*60}")
        print(f"ì§ˆë¬¸: {query}")
        print(f"{'='*60}")

        result = chatbot.chat(query)

        print(f"\në‹µë³€:\n{result['answer']}\n")
        print(f"ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê´€ë ¨: {result['is_smartstore_related']}")
        print(f"\ní›„ì† ì§ˆë¬¸:")
        for i, fq in enumerate(result['follow_up_questions'], 1):
            print(f"  {i}. {fq}")

        if result['sources']:
            print(f"\nì°¸ê³  ë¬¸ì„œ:")
            for i, src in enumerate(result['sources'], 1):
                print(f"  {i}. [{src['category']}] {src['question'][:50]}...")
